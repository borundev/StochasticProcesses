{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go into stochastic calculus let's quickly review what ordinary calculus is. Let us consider a function $G(t)$ over an interval $(t_0,t_n=t)$. We partition this interval into n subintervals $(t_0,t_1),(t_1,t_2) \\dots (t_{n-1},t_n)$ and a set of points $\\tau_i$ inside these intervals (i.e. $t_{i-1} < \\tau_i < t_i$) and then we define\n",
    "\n",
    "$$\n",
    "\\int_0^{t_n} G(t') dt' = \\lim_{n \\to \\infty} \\sum_{i=1}^n G(\\tau_i) [ t_i - t_{i-1}]~.\n",
    "$$\n",
    "\n",
    "Now in the finite n case there is some ambiguity in the above expression but in the limit of infinite $n$ this ambiguity goes away on account of the function $G(t')$ being differentiable. This is actually a circular statement in that we can only define the above opration un-ambiguously when the above limit exists.\n",
    "\n",
    "However, when we are talking about a stochastic process, by definition it cannot be differentiable. We saw this explicitely when we [approached the Wiener process from the limit of a random walk in an earlier chapter](RandomWalkAndWeinerProcess.ipynb). Intuitively this happens because if the process is fundamentally stochastic, i.e. it is stochastic to the smallest time step we can imagine (note this is not the case for random walk because during any given step the motion is smooth but this is the case for the Weiner process) then the first derivative is discontinuous.\n",
    "\n",
    "So how do we define a calculus for stochastic processes? We saw some hints of this in the [earlier chapter on random walks and Wiener process](RandomWalkAndWeinerProcess.ipynb) and will now see it more formally. First we make the convergence condition weaker to accomodate the non-differentiable nature in the following way\n",
    "\n",
    "$$\n",
    "\\int_0^{t_n} G(t') dW(t') = \\text{ms-}\\lim_{n \\to \\infty} \\sum_{i=1}^n G(\\tau_i) [ W(t_i) - W(t_{i-1})]~.\n",
    "$$\n",
    "\n",
    "where the mean-square limit is defined as\n",
    "\n",
    "$$\n",
    "\\text{ms-}\\lim_{n \\to \\infty} X_n = X \\Leftrightarrow \\lim_{n \\to \\infty} \\langle (X_n - X)^2 \\rangle=0\n",
    "$$\n",
    "\n",
    "This however does not uniquely fix the integral. The residual ambiguity is due to the fact that we can choose $\\tau_i = \\alpha t_i + (1-\\alpha) t_{i-1}$ and get different results. This easy to verify by evaluating the mean of $S=\\int W(t') dW(t')$. Being a mean it has a definitie (non-stochastic) value and we demonstrate below that even in the limit $n \\to \\infty$ the answer dependends on $\\alpha$.\n",
    "\n",
    "We calculate\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\langle S_n \\rangle &=& \\sum_{i=1}^n \\langle W(\\tau_i) W(t_i)\\rangle - \\langle W(\\tau_i) W(t_{i-1}) \\rangle \\\\\n",
    "&=& \\sum_{i=1}^n \\tau_i - t_{i-1} \\rangle \\\\\n",
    "&=& \\sum_{i=1}^n \\alpha (t_i - t_{i-1}) \\\\\n",
    "&=& \\alpha(t- t_0)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "where we used the relation $cov(W_t, W_{t'}) = min(t,t')$ explained in the [chapter on Weiner processes](RandomWalkAndWeinerProcess.ipynb). Thus we see the answer depends on $\\alpha$ independent of how big $n$ is. The choice $\\alpha=0$ defines Ito's stochastic integral and the choice $\\alpha=1$ defines Stratonovich's stochastic integral. \n",
    "\n",
    "Ito's calculus has the nice property of being causal in that the increment to the integral in the time interval  $(t_{i-1}, t_i)$ needs to know only about the value of the integrand at the beginning of the interval $G(t_{i-1})$. This translates into the integral being Martingales and especially useful in finance. The causality property is useful in physical applications also. As of writing this I am not aware of applications that are natuarally bettwe described using Stratonovich's definition.\n",
    "\n",
    "\n",
    "Now we can look at an application of Ito's calculus with the explicit summation method before we discuss the more formal Ito's lemma that gives answers magically as it were. We will later that Ito's calculus gives us $\\int_0^t W(s) dW(s) = \\frac{1}{2} (W(t)^2 - t)$. Here we see how this is consistent with the summation under the mean-square limit. This is based on an answer on [quant.stackexchange.com](http://quant.stackexchange.com/questions/25019/intergral-of-brownian-motion-w-r-t-brownian-motion/25051#25051). Let us evaluate the expression we would get on the RHS when evaluating $\\int_0^t W(s) dW(s)$ from the above definition:\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "Y^{(1)}_n &=& \\sum_{i=1}^n W_{t_{i-1}} (W_{t_i} - W_{t_{i-1}})~.\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "Now noting each increment $(W_{t_i} - W_{t_{i-1}}) = \\sqrt{dt} \\mathcal N_{i-1}(0,1)$ is an **independent normal** distribution we get\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "Y^{(1)}_n &=& dt \\sum_{i > j} \\mathcal N_i(0,1) \\mathcal N_j(0,1)~.\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "The other expression we get from the discretized version of the RHS of the standard answer $\\int_0^t W(s) dW(s)=\\frac{1}{2} (W(t)^2 - t)$ is\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "Y^{(2)}_n &=& \\frac{1}{2} \\left(\\sqrt{dt} \\sum_{i =1}^n \\mathcal N_i(0,1)\\right)^2 -  \\frac{1}{2} \\sum_{i=1}^n dt~.\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Now if we can just show that the two expressions are same in the mean-square limit we are done. We see that\n",
    "$$\\begin{eqnarray}\n",
    "&&\\lim_{n \\to \\infty} \\langle (Y^{(2)}_n - Y^{(1)}_n)^2 \\rangle \\\\\n",
    "&=&\\lim_{n \\to \\infty} \\frac{dt^2}{4}  \\langle \\left(\\sum_{i=1}^n \\mathcal N_i(0,1)^2 -n\\right)^2 \\rangle \\\\\n",
    "&=& \\lim_{n \\to \\infty} \\frac{dt^2}{4} \\langle (\\chi_n^2 -n)^2 \\rangle \\\\\n",
    "&=& \\lim_{n \\to \\infty} \\frac{dt^2}{4} var(\\chi_n^2)\\\\\n",
    "&=& \\lim_{n \\to \\infty} \\frac{dt^2}{2} n \\\\\n",
    "&=& \\lim_{n \\to \\infty}  \\frac{t^2}{2n} \\\\\n",
    "&=& 0\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "Here we used the results that the squares of $n$ standard normals is a chi-square with degree $n$ and that the same has mean $n$ and variance $2n$. \n",
    "\n",
    "Hence we are able to prove in detail why $\\int_0^t W(s) dW(s)=\\frac{1}{2} (W(t)^2 - t)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ito's Lemma\n",
    "\n",
    "We note in passing that $dW^2$ is a $\\chi_1^2$ distribution with mean $dt$ and standard deviation $\\sqrt{2}dt$ and thus in the limit $dt \\to 0$ we have $dW^2 \\to dt$. Then we get\n",
    "\n",
    "$$\n",
    "d f[W(t),t]=\\left[\\frac{\\partial f}{\\partial t} + \\frac{1}{2} \\frac{\\partial^2 f}{\\partial W^2} \\right] dt+ \\frac{\\partial f}{\\partial W} dW(t) + \\mathcal O(dt^{3/2})\n",
    "$$\n",
    "\n",
    "and obvious generalizations for more independent Weiner processes. This allows us to verify the above result for the integral of the Weiner process. Take $f(W(t),t) = \\frac{1}{2} (W(t)^2 - t)$ and apply the above rule to get the RHS as $W(t) dW(t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ito's Isometry\n",
    "\n",
    "Ito's isometry is the following result\n",
    "\n",
    "$$\n",
    "E[ (\\int X dW)^2] = E[\\int X^2 dt]~.\n",
    "$$\n",
    "\n",
    "This can be proved in the following way. The LHS is the mean square limit of the sum\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "&& \\langle (\\sum_{i=1}^n X(t_{i-1}) (W_{t_i} - W_{t_{i-1}}))^2 \\rangle \\\\\n",
    "&=&  \\sum_{i,j=1}^n \\langle X(t_{i-1}) X(t_{j-1}) (W_{t_i} - W_{t_{i-1}}) (W_{t_j} - W_{t_{j-1}}) \\rangle\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "now due the statistical independence of $X_{t_{i-1}}$ with $(W_{t_i} - W_{t_{i-1}})$, the above expenctation value factorizes\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "&&  \\sum_{i,j=1}^n \\langle X(t_{i-1}) X(t_{j-1})\\rangle \\langle (W_{t_i} - W_{t_{i-1}}) (W_{t_j} - W_{t_{j-1}}) \\rangle\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "and now we use the result $\\langle (W_{t_i} - W_{t_{i-1}}) (W_{t_j} - W_{t_{j-1}}) \\rangle = \\delta_{ij}dt $ to get the advertized result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating Stochastic Integrals\n",
    "\n",
    "\n",
    "Suppose we have a stochastic process\n",
    "\n",
    "$$\n",
    "dS=a(S) dt  + b(S) dW\n",
    "$$\n",
    "\n",
    "then we can get the following result\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\Delta S &=& \\frac{\\partial S}{\\partial W} \\Delta W + \\frac{1}{2} \\frac{\\partial^2 S}{\\partial W^2} \\Delta W^2 + \\frac{\\partial S}{\\partial t} \\Delta t + \\mathcal O(\\Delta W^3) \\\\\n",
    "&=& \\Big( \\frac{\\partial S}{\\partial t}+\\frac{1}{2} \\frac{\\partial^2 S}{\\partial W^2} \\Big) \\Delta t + \\frac{\\partial S}{\\partial W} \\Delta W + \\frac{1}{2} \\frac{\\partial^2 S}{\\partial W^2} (\\Delta W^2 - \\Delta t) + \\mathcal O(\\Delta W^3)\\\\\n",
    "&=& a(S) \\Delta t + b(S) \\Delta W + \\frac{1}{2} b(S) b'(S) ( \\Delta W^2 - \\Delta t) + \\mathcal O(\\Delta W^3)\n",
    "\\end{eqnarray}\n",
    "\n",
    "This method goes under the name of [Milstein-method](https://en.wikipedia.org/wiki/Milstein_method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
